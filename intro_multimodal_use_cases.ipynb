{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQT500QqVPIb"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "This notebook demonstrates a variety of multimodal use cases that Gemini can be used for.\n",
        "\n",
        "#### Multimodal use cases\n",
        "\n",
        "Compared to text-only LLMs, Gemini 1.5's multimodality can be used for many new use-cases:\n",
        "\n",
        "Example use cases with **text and image(s)** as input:\n",
        "\n",
        "- Detecting objects in photos\n",
        "- Understanding screens and interfaces\n",
        "- Understanding of drawing and abstraction\n",
        "- Understanding charts and diagrams\n",
        "- Recommendation of images based on user preferences\n",
        "- Comparing images for similarities, anomalies, or differences\n",
        "\n",
        "Example use cases with **text and video** as input:\n",
        "\n",
        "- Generating a video description\n",
        "- Extracting tags of objects throughout a video\n",
        "- Extracting highlights/messaging of a video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhsUe0fyc-ER"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDU0XJ1xRDlL"
      },
      "source": [
        "## Getting Started\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5afkyDMSBW5"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc4WxYmLSBW5"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --user google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart current runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fom0ZkMSBW6"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCaCx6PLSBW6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGB8Txa_e4V0"
      },
      "source": [
        "### Define Google Cloud project information and initialize Vertex AI\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuQwwRiniVFG"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTk488WDPBtQ"
      },
      "outputs": [],
      "source": [
        "from vertexai.generative_models import GenerationConfig, GenerativeModel, Image, Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7rZuTClfNs0"
      },
      "source": [
        "## Use the Gemini 1.5 Flash model\n",
        "\n",
        "Gemini 1.5 Flash (`gemini-1.5-flash`) is a multimodal model that supports multimodal prompts. You can include text, image(s), and video in your prompt requests and get text or code responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTNnM-lqfQRo"
      },
      "source": [
        "### Load Gemini 1.5 Flash model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2998506fe6d1"
      },
      "outputs": [],
      "source": [
        "multimodal_model = GenerativeModel(\"gemini-1.5-flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpL3OkSCfIAR"
      },
      "source": [
        "### Define helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7QMAHXse339"
      },
      "outputs": [],
      "source": [
        "import http.client\n",
        "import typing\n",
        "import urllib.request\n",
        "\n",
        "import IPython.display\n",
        "from PIL import Image as PIL_Image\n",
        "from PIL import ImageOps as PIL_ImageOps\n",
        "\n",
        "\n",
        "def display_images(\n",
        "    images: typing.Iterable[Image],\n",
        "    max_width: int = 600,\n",
        "    max_height: int = 350,\n",
        ") -> None:\n",
        "    for image in images:\n",
        "        pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
        "        if pil_image.mode != \"RGB\":\n",
        "            # RGB is supported by all Jupyter environments (e.g. RGBA is not yet)\n",
        "            pil_image = pil_image.convert(\"RGB\")\n",
        "        image_width, image_height = pil_image.size\n",
        "        if max_width < image_width or max_height < image_height:\n",
        "            # Resize to display a smaller notebook image\n",
        "            pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
        "        IPython.display.display(pil_image)\n",
        "\n",
        "\n",
        "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
        "    with urllib.request.urlopen(image_url) as response:\n",
        "        response = typing.cast(http.client.HTTPResponse, response)\n",
        "        image_bytes = response.read()\n",
        "    return image_bytes\n",
        "\n",
        "\n",
        "def load_image_from_url(image_url: str) -> Image:\n",
        "    image_bytes = get_image_bytes_from_url(image_url)\n",
        "    return Image.from_bytes(image_bytes)\n",
        "\n",
        "\n",
        "def display_content_as_image(content: str | Image | Part) -> bool:\n",
        "    if not isinstance(content, Image):\n",
        "        return False\n",
        "    display_images([content])\n",
        "    return True\n",
        "\n",
        "\n",
        "def display_content_as_video(content: str | Image | Part) -> bool:\n",
        "    if not isinstance(content, Part):\n",
        "        return False\n",
        "    part = typing.cast(Part, content)\n",
        "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
        "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
        "    IPython.display.display(IPython.display.Video(video_url, width=600))\n",
        "    return True\n",
        "\n",
        "\n",
        "def print_multimodal_prompt(contents: list[str | Image | Part]):\n",
        "    \"\"\"\n",
        "    Given contents that would be sent to Gemini,\n",
        "    output the full multimodal prompt for ease of readability.\n",
        "    \"\"\"\n",
        "    for content in contents:\n",
        "        if display_content_as_image(content):\n",
        "            continue\n",
        "        if display_content_as_video(content):\n",
        "            continue\n",
        "        print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OWurhO4mu4J"
      },
      "source": [
        "## Image understanding across multiple images\n",
        "\n",
        "One of the capabilities of Gemini is being able to reason across multiple images.\n",
        "\n",
        "This is an example of using Gemini to calculate the total cost of groceries using an image of fruits and a price list:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyRoVquPmy9H"
      },
      "outputs": [],
      "source": [
        "image_grocery_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/banana-apple.jpg\"\n",
        "image_prices_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pricelist.jpg\"\n",
        "image_grocery = load_image_from_url(image_grocery_url)\n",
        "image_prices = load_image_from_url(image_prices_url)\n",
        "\n",
        "instructions = \"Instructions: Consider the following image that contains fruits:\"\n",
        "prompt1 = \"How much should I pay for the fruits given the following price list?\"\n",
        "prompt2 = \"\"\"\n",
        "Answer the question through these steps:\n",
        "Step 1: Identify what kind of fruits there are in the first image.\n",
        "Step 2: Count the quantity of each fruit.\n",
        "Step 3: For each grocery in first image, check the price of the grocery in the price list.\n",
        "Step 4: Calculate the subtotal price for each type of fruit.\n",
        "Step 5: Calculate the total price of fruits using the subtotals.\n",
        "\n",
        "Answer and describe the steps taken:\n",
        "\"\"\"\n",
        "\n",
        "contents = [\n",
        "    instructions,\n",
        "    image_grocery,\n",
        "    prompt1,\n",
        "    image_prices,\n",
        "    prompt2,\n",
        "]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Response--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy-me3PdgMUH"
      },
      "source": [
        "## Understanding Screens and Interfaces\n",
        "\n",
        "Gemini can also extract information from appliance screens, UIs, screenshots, icons, and layouts.\n",
        "\n",
        "For example, if you input an image of a stove, you can ask Gemini to provide instructions to help a user navigate the UI and respond in different languages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDjN4thV8orx"
      },
      "outputs": [],
      "source": [
        "image_stove_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/stove.jpg\"\n",
        "image_stove = load_image_from_url(image_stove_url)\n",
        "\n",
        "prompt = \"\"\"Help me to reset the clock on this appliance?\n",
        "Provide the instructions in English and French.\n",
        "If instructions include buttons, also explain where those buttons are physically located.\n",
        "\"\"\"\n",
        "\n",
        "contents = [image_stove, prompt]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Response--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbhfMO478orx"
      },
      "source": [
        "Note: The response may not be completely accurate, as the model may hallucinate; however, the model is able to identify the location of buttons and translate in a single query. To mitigate hallucinations, one approach is to ground the LLM with retrieval-augmented generation, which is outside the scope of this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4437b7608c8e"
      },
      "source": [
        "## Understanding entity relationships in technical diagrams\n",
        "\n",
        "Gemini has multimodal capabilities that enable it to understand diagrams and take actionable steps, such as optimization or code generation. This example demonstrates how Gemini can decipher an entity relationship (ER) diagram, understand the relationships between tables, identify requirements for optimization in a specific environment like BigQuery, and even generate corresponding code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klY4yBEiKmET"
      },
      "outputs": [],
      "source": [
        "image_er_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/er.png\"\n",
        "image_er = load_image_from_url(image_er_url)\n",
        "\n",
        "prompt = \"Document the entities and relationships in this ER diagram.\"\n",
        "\n",
        "contents = [prompt, image_er]\n",
        "\n",
        "# Use a more deterministic configuration with a low temperature\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    candidate_count=1,\n",
        "    max_output_tokens=2048,\n",
        ")\n",
        "\n",
        "responses = multimodal_model.generate_content(\n",
        "    contents,\n",
        "    generation_config=generation_config,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Response--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXrvQxpiQKp6"
      },
      "source": [
        "## Recommendations based on multiple images\n",
        "\n",
        "Gemini is capable of image comparison and providing recommendations. This may be useful in industries like e-commerce and retail.\n",
        "\n",
        "Below is an example of choosing which pair of glasses would be better suited to an oval-shaped face:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7M7B9q7L1X7"
      },
      "outputs": [],
      "source": [
        "image_glasses1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses1.jpg\"\n",
        "image_glasses2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses2.jpg\"\n",
        "image_glasses1 = load_image_from_url(image_glasses1_url)\n",
        "image_glasses2 = load_image_from_url(image_glasses2_url)\n",
        "\n",
        "prompt1 = \"\"\"\n",
        "Which of these glasses you recommend for me based on the shape of my face?\n",
        "I have an oval shape face.\n",
        "----\n",
        "Glasses 1:\n",
        "\"\"\"\n",
        "prompt2 = \"\"\"\n",
        "----\n",
        "Glasses 2:\n",
        "\"\"\"\n",
        "prompt3 = \"\"\"\n",
        "----\n",
        "Explain how you reach out to this decision.\n",
        "Provide your recommendation based on my face shape, and reasoning for each in JSON format.\n",
        "\"\"\"\n",
        "\n",
        "contents = [prompt1, image_glasses1, prompt2, image_glasses2, prompt3]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Response--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBrdsvIU7Zkf"
      },
      "source": [
        "## Similarity/Differences\n",
        "\n",
        "Gemini can compare images and identify similarities or differences between objects.\n",
        "\n",
        "The following example shows two scenes from Marienplatz in Munich, Germany that are slightly different. Gemini can compare between the images and find similarities/differences:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUSJduLh8457"
      },
      "outputs": [],
      "source": [
        "image_landmark1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/landmark1.jpg\"\n",
        "image_landmark2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/landmark2.jpg\"\n",
        "image_landmark1 = load_image_from_url(image_landmark1_url)\n",
        "image_landmark2 = load_image_from_url(image_landmark2_url)\n",
        "\n",
        "prompt1 = \"\"\"\n",
        "Consider the following two images:\n",
        "Image 1:\n",
        "\"\"\"\n",
        "prompt2 = \"\"\"\n",
        "Image 2:\n",
        "\"\"\"\n",
        "prompt3 = \"\"\"\n",
        "1. What is shown in Image 1? Where is it?\n",
        "2. What is similar between the two images?\n",
        "3. What is difference between Image 1 and Image 2 in terms of the contents or people shown?\n",
        "\"\"\"\n",
        "\n",
        "contents = [prompt1, image_landmark1, prompt2, image_landmark2, prompt3]\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    candidate_count=1,\n",
        "    max_output_tokens=2048,\n",
        ")\n",
        "\n",
        "responses = multimodal_model.generate_content(\n",
        "    contents,\n",
        "    generation_config=generation_config,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Response--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN8nVlITK5kz"
      },
      "source": [
        "## Generating a video description\n",
        "\n",
        "Gemini can also extract tags throughout a video:\n",
        "\n",
        "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT2nArvxZv-P"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "What is shown in this video?\n",
        "Where should I go to see it?\n",
        "What are the top 5 places in the world that look like this?\n",
        "\"\"\"\n",
        "video = Part.from_uri(\n",
        "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "contents = [prompt, video]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Response--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9RdLpH128Ao"
      },
      "source": [
        "> You can confirm that the location is indeed Antalya, Turkey by visiting the Wikipedia page: https://en.wikipedia.org/wiki/Antalya\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksjZiIfnO0zQ"
      },
      "source": [
        "## Extracting tags of objects throughout the video\n",
        "\n",
        "Gemini can also extract tags throughout a video.\n",
        "\n",
        "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/photography.mp4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9qE2GGIA975"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Answer the following questions using the video only:\n",
        "- What is in the video?\n",
        "- What is the action in the video?\n",
        "- Provide 10 best tags for this video?\n",
        "\"\"\"\n",
        "video = Part.from_uri(\n",
        "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/photography.mp4\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "contents = [prompt, video]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Response--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiQzvIJfQxbQ"
      },
      "source": [
        "## Asking more questions about a video\n",
        "\n",
        "Below is another example of using Gemini to ask questions the video and return a JSON response.\n",
        "\n",
        "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4  \n",
        "> _Note: Although this video contains audio, Gemini does not currently support audio input and will only answer based on the video._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQIV9SwCz5WM"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Answer the following questions using the video only:\n",
        "What is the profession of the main person?\n",
        "What are the main features of the phone highlighted?\n",
        "Which city was this recorded in?\n",
        "Provide the answer JSON.\n",
        "\"\"\"\n",
        "video = Part.from_uri(\n",
        "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "contents = [prompt, video]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Response--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LurOmNuRpDr"
      },
      "source": [
        "## Retrieving extra information beyond the video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17A1dteeJut-"
      },
      "source": [
        "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CY-zlixU87O"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Which line is this?\n",
        "where does it go?\n",
        "What are the stations/stops of this line?\n",
        "\"\"\"\n",
        "video = Part.from_uri(\n",
        "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "contents = [prompt, video]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Response--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZrxMm_83Vps"
      },
      "source": [
        "> You can confirm that this is indeed the Confederation Line on Wikipedia here: https://en.wikipedia.org/wiki/Confederation_Line\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_multimodal_use_cases.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
